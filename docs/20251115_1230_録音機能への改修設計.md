# 録音機能への改修設計

## Phase 1の振り返り

### 実装した内容
- テキストパーサー（カタカナ → リズムセグメント）
- リズム量子化ロジック（16分音符単位）
- VexFlowによる五線譜描画
- 基本的なReact UI

### 課題と反省点
1. **テキスト入力の限界**
   - 「トゥーダッタ」のような入力では、ユーザーの意図したリズムと結果のギャップが大きい
   - 文字数ベースの長さ判定では、微妙なリズムのニュアンスを表現できない
   - 実際の音楽的タイミングと乖離してしまう

2. **より直感的な入力方法の必要性**
   - 録音（鼻歌、手拍子、口ドラムなど）の方が自然
   - 実際の音のタイミングを直接取得できる

## Phase 2: 録音からのリズム検出

### 概要
マイク入力から音声を録音し、オンセット検出によってリズムパターンを抽出する。

### 技術要件

#### 1. 録音機能（Web Audio API）
- **使用API**: MediaStream API + Web Audio API
- **サンプリングレート**: 44.1kHz
- **録音形式**: モノラル
- **最大録音時間**: 10秒（初期実装）

#### 2. 音声解析
- **解析方法**:
  - エネルギーベースのオンセット検出
  - RMS（Root Mean Square）による音量計算
  - しきい値を超えた時点を「音の開始」として検出

- **パラメータ**:
  - フレームサイズ: 2048サンプル（約46ms）
  - ホップサイズ: 512サンプル（約12ms）
  - オンセット検出しきい値: 動的に調整（録音の最大音量の30%など）

#### 3. リズム量子化
- **グリッド解像度**: 16分音符（既存実装を流用）
- **量子化方法**: 最近傍グリッドへのスナップ
- **BPM推定**:
  - Phase 2-1: ユーザー指定（デフォルト120）
  - Phase 2-2: 自動推定（オンセット間隔から推定）

#### 4. データフロー
```
[マイク入力]
    ↓
[MediaStream API: 録音]
    ↓
[AudioBuffer: 生データ]
    ↓
[オンセット検出: 音の開始時刻を抽出]
    ↓
[時間 → ビート変換: BPMを使って秒からビートへ]
    ↓
[量子化: 16分音符グリッドにスナップ]
    ↓
[RhythmNote配列: startBeat, durationBeats]
    ↓
[五線譜描画: VexFlow]
```

### 実装の優先順位

#### Step 1: 基本的な録音機能
- [ ] マイク録音UI（録音開始/停止ボタン）
- [ ] MediaStream APIで音声キャプチャ
- [ ] AudioBufferに変換
- [ ] 波形の可視化（デバッグ用）

#### Step 2: オンセット検出
- [ ] RMS計算関数
- [ ] しきい値ベースのオンセット検出
- [ ] 検出結果の時刻リストを生成

#### Step 3: リズム変換
- [ ] 時刻 → ビート変換（BPM指定）
- [ ] 既存の量子化ロジックと統合
- [ ] 音の長さの推定（次のオンセットまでの時間）

#### Step 4: UI改善
- [ ] 録音中のフィードバック（入力レベルメーター）
- [ ] BPM設定UI
- [ ] 再録音機能
- [ ] テキスト入力と録音入力の切り替え

### オンセット検出アルゴリズム（詳細）

```typescript
function detectOnsets(audioBuffer: AudioBuffer, threshold: number): number[] {
  const samples = audioBuffer.getChannelData(0)
  const frameSize = 2048
  const hopSize = 512
  const sampleRate = audioBuffer.sampleRate

  const onsets: number[] = []
  let previousEnergy = 0

  for (let i = 0; i < samples.length - frameSize; i += hopSize) {
    // フレーム内のRMSを計算
    let sumSquares = 0
    for (let j = 0; j < frameSize; j++) {
      sumSquares += samples[i + j] ** 2
    }
    const rms = Math.sqrt(sumSquares / frameSize)

    // エネルギーの急激な上昇を検出
    const energyIncrease = rms - previousEnergy
    if (energyIncrease > threshold) {
      const timeInSeconds = i / sampleRate
      onsets.push(timeInSeconds)
    }

    previousEnergy = rms
  }

  return onsets
}
```

### 期待される改善点
1. **直感性**: 実際に歌ったり叩いたりしたリズムをそのまま入力
2. **正確性**: タイミングの微妙な違いも反映
3. **柔軟性**: 様々な音源（鼻歌、手拍子、口ドラムなど）に対応

### リスクと対策

#### リスク1: ノイズ検出
- **問題**: 環境音や息継ぎなどを誤検出
- **対策**:
  - しきい値の調整UI提供
  - ノイズゲート機能
  - 検出結果のプレビューと手動修正機能

#### リスク2: BPM推定の精度
- **問題**: 自動BPM推定が不正確
- **対策**:
  - まずはユーザー指定から実装
  - メトロノーム機能（録音前にテンポを刻む）

#### リスク3: ブラウザ互換性
- **問題**: マイク許可やWeb Audio APIのサポート
- **対策**:
  - モダンブラウザのみサポート（Chrome, Firefox, Safari最新版）
  - フォールバックとしてテキスト入力を残す

## Phase 3以降の展望
- MIDI出力機能
- リズムパターンのライブラリ保存
- 複数トラックのサポート
- スウィング・グルーヴ調整
- 音高検出（ピッチ検出）の追加
